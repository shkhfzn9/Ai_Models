cat <<EOF > README.md

# ğŸ§  DialogSum Summarizer using Pegasus

This project implements a dialogue summarizer using the **Pegasus** transformer model fine-tuned on the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) dataset. The model is trained and evaluated using the Hugging Face \`transformers\` and \`datasets\` libraries.

---

## ğŸ”§ Author

Sheikh Faizan

## ğŸ› ï¸ Framework

Hugging Face Transformers

## ğŸ’» Dataset

DialogSum

## ğŸ¤– Model

google/pegasus-cnn_dailymail

---

## ğŸ“ Project Structure

\`\`\`
dialogsum_summarizer/
â”œâ”€â”€ dialogsum_summarizer.py # Training and testing pipeline
â”œâ”€â”€ pegasus-dialogsum-final/ # Directory for saving trained model
â”œâ”€â”€ README.md # You're reading it now
\`\`\`

---

## ğŸ“Œ Features

- âœ… Fine-tune \`google/pegasus-cnn_dailymail\` on \`DialogSum\` dialogue data.
- âœ… Generates abstractive summaries from dialogue input.
- âœ… Trains with Hugging Face \`Trainer\` API.
- âœ… Evaluation using ROUGE metrics.
- âœ… Works on both GPU (CUDA) and CPU.
- âœ… Easily test any sample from the test dataset.

---

## ğŸ“¦ Installation

1. Create a virtual environment (optional):

\`\`\`bash
python -m venv venv
source venv/bin/activate # or venv\\Scripts\\activate on Windows
\`\`\`

2. Install dependencies:

\`\`\`bash
pip install transformers datasets evaluate tqdm pandas torch
\`\`\`

---

## ğŸš€ Usage

### ğŸ”§ Train the model

\`\`\`bash
python dialogsum_summarizer.py --train
\`\`\`

### ğŸ“ˆ Test with a specific sample

\`\`\`bash
python dialogsum_summarizer.py --test 0
\`\`\`

---

## ğŸ§ª Evaluation

Evaluation is done using ROUGE metrics:

- ROUGE-1
- ROUGE-2
- ROUGE-L
- ROUGE-Lsum

---

## ğŸ“Š Example Output

\`\`\`text
Input Dialogue:
Speaker 1: Hey, how are you?
Speaker 2: I'm fine, thanks. How about you?

Reference Summary:
Two friends catch up and exchange pleasantries.

Generated Summary:
Two people greet each other and ask about each other's well-being.
\`\`\`

---

## ğŸ“Œ Notes

- Model: \`google/pegasus-cnn_dailymail\`
- Dataset: DialogSum
- Tokenization: Max input = 1024, Max summary = 128
- Batch size: 1
- Epochs: 1

---

## ğŸ“‚ Save Model

The model and tokenizer will be saved to:

\`\`\`
pegasus-dialogsum-final/
\`\`\`

---

## ğŸ“¬ Contact

- GitHub: [shkhfzn9](https://github.com/shkhfzn9)
- LinkedIn: [Sheikh Faizan](https://www.linkedin.com/in/sheikh-faizan-4a9a29326/)
- Email: Sheikhfaizan.w@gmail.com

---

## ğŸ“œ License

MIT License

---

# ğŸ‹ï¸â€â™‚ï¸ My AI Model Training

## Training Results

### Training Loss

![Training Loss](screenshots/trainingLoss.png)

### Accuracy Plots and Training Details

#### Data Conversion Process

![](screenshots/dataConverstion.png)

#### Dataset Features, Rows, and Overview

![](screenshots/datasetFeaturesRowsEtc.png)

#### Generated Conversation Example Using Model Training

![](screenshots/generatedConversationUsingModelTraining.png)

#### Loading the Dataset

![](screenshots/LoadingDataSet.png)

#### Saving the Trained Model and Tokenizer

![](screenshots/SavingTheTrainingModelAndTokenizer.png)

#### Training Data Output Without Model

![](screenshots/traingDataOutPutWithoutModel.png)

EOF
